//
//  UV-TM.swift
//  VoiceApp
//
//  Created by Amedeo Ercole on 7/18/25.
//
import SwiftUI
import Combine
import AVFoundation

// Data model for live match information
struct LiveMatchData {
    let matchId: String
    let sessionId: String
    let roomId: String
    let livekitToken: String
    let participants: [MatchParticipant]
    let topics: [String]
    let hashtags: [String]
}

struct MatchParticipant {
    let userId: String
    let displayName: String
    let isCurrentUser: Bool
}

// ç®€åŒ–åŽçš„è§†å›¾ï¼Œåªä¿ç•™æ ¸å¿ƒåŠŸèƒ½
struct UserVoiceTopicMatchingView: View {
    let matchResult: MatchResult
    @Environment(\.dismiss) private var dismiss
    
    // AIæœåŠ¡ï¼Œå¤„ç†æ‰€æœ‰WebSocketå’ŒéŸ³é¢‘é€»è¾‘
    @StateObject private var aiVoiceService = AIVoiceService()
    
    // Navigation state for when match is found
    @State private var navigateToLiveChat = false
    @State private var matchData: LiveMatchData?

    var body: some View {
        ZStack {
            // èƒŒæ™¯
            Color.black.ignoresSafeArea()

            // é¡¶éƒ¨UIå…ƒç´ 
            VStack {
                // è¿”å›žæŒ‰é’®
                HStack {
                    Button(action: {
                        // åœæ­¢AIæœåŠ¡å¹¶è¿”å›ž
                        aiVoiceService.cleanup()
                        dismiss()
                    }) {
                        Image(systemName: "arrow.left")
                            .font(.title2)
                            .foregroundColor(.white)
                    }
                    Spacer()
                }
                .padding()
                
                Spacer()
                
                // AIå›žå¤çš„æ–‡å­—æ˜¾ç¤ºåŒºåŸŸ
                ScrollView {
                    Text(aiVoiceService.currentResponse)
                        .font(.custom("Rajdhani", size: 28))
                        .foregroundColor(.white)
                        .multilineTextAlignment(.center)
                        .padding()
                }
                
                Spacer()
            }
            
            // åº•éƒ¨éº¦å…‹é£ŽæŒ‰é’®
            VStack {
                Spacer()
                
                Button(action: {
                    // åˆ‡æ¢é™éŸ³çŠ¶æ€
                    aiVoiceService.toggleMute()
                }) {
                    Image(systemName: aiVoiceService.isMuted ? "mic.slash.fill" : "mic.fill")
                        .font(.system(size: 40))
                        .foregroundColor(aiVoiceService.isMuted ? .red : .white)
                        .padding(20)
                        .background(Circle().fill(Color.white.opacity(0.2)))
                }
                
                Text(aiVoiceService.isMuted ? "Muted" : "Listening...")
                    .foregroundColor(.white)
                    .padding(.bottom, 30)
            }
        }
        .onAppear {
            // è§†å›¾å‡ºçŽ°æ—¶åˆå§‹åŒ–AIå¯¹è¯
            Task {
                await aiVoiceService.initializeAIConversation(with: matchResult)
            }
        }
        .navigationBarHidden(true)
        // Navigation to live chat when match is found
        .background(
            NavigationLink(
                destination: matchData.map { data in
                    HashtagScreen(matchData: data)
                },
                isActive: $navigateToLiveChat
            ) {
                EmptyView()
            }
            .hidden()
        )
        // Listen for match found events
        .onReceive(aiVoiceService.$matchFound) { matchData in
            if let matchData = matchData {
                self.matchData = matchData
                self.navigateToLiveChat = true
            }
        }
    }
}

// MARK: - AI è¯­éŸ³æœåŠ¡ - GPT-4o Realtime WebSocket
class AIVoiceService: NSObject, ObservableObject, WebSocketDelegate, AVAudioPlayerDelegate {
    @Published var isConnected = false
    @Published var isListening = false
    @Published var isMuted = false
    @Published var currentResponse = ""
    @Published var isAISpeaking = false
    @Published var matchFound: LiveMatchData?
    
    private var matchContext: MatchResult?
    private var conversationContext: String = ""
    private var webSocketService: WebSocketService?
    private var authToken: String?
    private var isAuthenticated = false
    private var sessionStarted = false
    
    // éŸ³é¢‘ç›¸å…³
    private var audioEngine: AVAudioEngine?
    private var inputNode: AVAudioInputNode?
    private var isRecording = false
    private var audioStartTime: Date?
    private var audioChunkIndex: Int = 0
    
    // ðŸ”§ ä¿®å¤åŽçš„ç»Ÿä¸€éŸ³é¢‘æ’­æ”¾ç³»ç»Ÿ
    private var audioPlaybackQueue = DispatchQueue(label: "audio.playback", qos: .userInitiated)
    private var currentAudioPlayer: AVAudioPlayer?
    private var audioQueue: [Data] = [] // æŽ’é˜Ÿç­‰å¾…æ’­æ”¾çš„éŸ³é¢‘æ•°æ®
    private var isPlayingAudio = false
    private var audioAccumulator = Data() // ç´¯ç§¯å•ä¸ªå®Œæ•´å“åº”çš„æ‰€æœ‰éŸ³é¢‘å—
    
    override init() {
        // èŽ·å–è®¤è¯ä»¤ç‰Œ
        authToken = AuthService.shared.firebaseToken
        super.init()
        setupAudioSession()
        setupAudioEngine()
    }
    
    private func setupAudioSession() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.defaultToSpeaker, .allowBluetooth])
            
            // è®¾ç½®é¦–é€‰é‡‡æ ·çŽ‡ä¸º24kHzåŒ¹é…GPT-4o
            try audioSession.setPreferredSampleRate(24000)
            try audioSession.setActive(true)
            
            print("âœ… [AIVoice] Audio session configured for 24kHz voice chat (GPT-4o compatible)")
        } catch {
            print("âŒ [AIVoice] Audio session setup failed: \(error)")
        }
    }
    
    private func setupAudioEngine() {
        audioEngine = AVAudioEngine()
        inputNode = audioEngine?.inputNode
        
        guard let audioEngine = audioEngine,
              let inputNode = inputNode else {
            print("âŒ [AIVoice] Failed to setup audio engine")
            return
        }
        
        // ä½¿ç”¨ç¡¬ä»¶çš„å®žé™…è¾“å…¥æ ¼å¼
        let inputFormat = inputNode.inputFormat(forBus: 0)
        print("ðŸŽ™ï¸ [AIVoice] Hardware input format: \(inputFormat)")
        
        // åˆ›å»ºç›®æ ‡æ ¼å¼ (24kHz, Int16, mono) - åŒ¹é…OpenAI Realtime APIè¦æ±‚
        guard let targetFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, 
                                             sampleRate: 24000,  // OpenAIè¦æ±‚24kHz
                                             channels: 1, 
                                             interleaved: false) else {
            print("âŒ [AIVoice] Failed to create target audio format")
            return
        }
        
        print("ðŸŽµ [AIVoice] Target format for OpenAI: 24kHz, PCM16, mono")
        
        // å®‰è£…tapä½¿ç”¨ç¡¬ä»¶çš„åŽŸç”Ÿæ ¼å¼
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { [weak self] buffer, time in
            self?.processAudioBuffer(buffer, originalFormat: inputFormat, targetFormat: targetFormat)
        }
        
        print("âœ… [AIVoice] Audio engine configured for streaming")
    }
    
    func initializeAIConversation(with matchResult: MatchResult) async {
        print("ðŸ¤– [AIVoice] Initializing AI conversation for session: \(generateSessionId())")
        
        self.matchContext = matchResult
        
        // è®¾ç½®å¯¹è¯ä¸Šä¸‹æ–‡
        conversationContext = """
        You are a friendly AI conversation partner in a voice chat app. The user wants to discuss these topics: \(matchResult.topics.joined(separator: ", ")).
        
        Their original message was: "\(matchResult.transcription)"
        
        Key guidelines:
        - You are NOT a matching algorithm or service
        - You are a conversation partner who enjoys discussing these topics
        - Keep responses conversational and engaging (1-3 sentences)
        - Ask follow-up questions to keep the conversation flowing
        - Use natural, spoken language (this is voice chat)
        - Don't mention "finding matches" or "waiting for others"
        - Focus on having an interesting discussion about the topics
        
        Hashtags for context: \(matchResult.hashtags.joined(separator: ", "))
        """
        
        print("ðŸ§  [AIVoice] AI conversation context set for topics: \(matchResult.topics)")
        
        // è¿žæŽ¥åˆ° WebSocket
        await connectToRealtimeAPI()
        
        print("âœ… [AIVoice] AI conversation initialized")
    }
    
    private func connectToRealtimeAPI() async {
        guard let token = authToken else {
            print("âŒ [AIVoice] No auth token available")
            return
        }
        
        print("ðŸ”Œ [AIVoice] Connecting to GPT-4o Audio Stream API...")
        
        await MainActor.run {
            // åˆ›å»º WebSocket æœåŠ¡
            webSocketService = WebSocketService()
            webSocketService?.delegate = self
            
            // è¿žæŽ¥åˆ°éŸ³é¢‘æµç«¯ç‚¹
            webSocketService?.connect(to: APIConfig.WebSocket.aiAudioStream, with: token)
        }
    }
    
    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer, originalFormat: AVAudioFormat, targetFormat: AVAudioFormat) {
        guard sessionStarted && !isMuted && isRecording else { 
            return 
        }
        
        // åˆ›å»ºéŸ³é¢‘è½¬æ¢å™¨
        guard let converter = AVAudioConverter(from: originalFormat, to: targetFormat) else {
            print("âŒ [AIVoice] Failed to create audio converter")
            return
        }
        
        // åˆ›å»ºè¾“å‡ºç¼“å†²åŒº
        let outputFrameCapacity = AVAudioFrameCount(Double(buffer.frameLength) * targetFormat.sampleRate / originalFormat.sampleRate)
        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: outputFrameCapacity) else {
            print("âŒ [AIVoice] Failed to create output buffer")
            return
        }
        
        // æ‰§è¡Œæ ¼å¼è½¬æ¢
        var error: NSError?
        _ = converter.convert(to: outputBuffer, error: &error) { _, outStatus in
            outStatus.pointee = .haveData
            return buffer
        }
        
        if let error = error {
            print("âŒ [AIVoice] Audio conversion failed: \(error)")
            return
        }
        
        // è½¬æ¢ä¸ºData
        guard let audioData = outputBuffer.toData() else {
            print("âŒ [AIVoice] Failed to convert converted buffer to data")
            return
        }
        
        // ç¼–ç ä¸ºbase64å¹¶å‘é€
        let base64Audio = audioData.base64EncodedString()
        
        let audioMessage: [String: Any] = [
            "type": "audio_chunk",
            "audio_data": base64Audio,
            "language": "en-US",
            "timestamp": Date().timeIntervalSince1970
        ]
        
        audioChunkIndex += 1
        webSocketService?.send(audioMessage)
        print("ðŸ“¤ [AIVoice] Sent audio chunk #\(audioChunkIndex): \(audioData.count) bytes")
    }
    
    private func sendStartSession() {
        let startSessionMessage: [String: Any] = [
            "type": "start_session",
            "user_context": [
                "topics": matchContext?.topics ?? [],
                "hashtags": matchContext?.hashtags ?? [],
                "transcription": matchContext?.transcription ?? "",
                "conversation_context": conversationContext
            ],
            "timestamp": Date().timeIntervalSince1970
        ]
        
        webSocketService?.send(startSessionMessage)
        print("ðŸ“¤ [AIVoice] Sent start session with topic context")
    }
    
    func startListening() async {
        guard sessionStarted && !isMuted else {
            print("âš ï¸ [AIVoice] Cannot start listening - session not started or muted")
            return
        }
        
        print("ðŸŽ¤ [AIVoice] Starting voice listening")
        
        await MainActor.run {
            isListening = true
        }
        
        startAudioEngine()
    }
    
    func toggleMute() {
        isMuted.toggle()
        print("ðŸ”‡ [AIVoice] Audio input \(isMuted ? "muted" : "unmuted")")
        
        if isMuted {
            stopAudioEngine()
        } else if sessionStarted {
            startAudioEngine()
        }
    }
    
    func cleanup() {
        print("ðŸ§¹ [AIVoice] Starting cleanup process")
        
        // Stop audio engine
        stopAudioEngine()
        
        // Stop any playing audio
        stopAllAudio()
        
        // Disconnect WebSocket
        webSocketService?.disconnect()
        webSocketService = nil
        
        // Reset all state
        isConnected = false
        isListening = false
        sessionStarted = false
        isAuthenticated = false
        
        // Clear audio data
        audioQueue.removeAll()
        audioAccumulator = Data()
        
        print("âœ… [AIVoice] Cleanup completed")
    }
    
    private func startAudioEngine() {
        guard !isMuted, let audioEngine = audioEngine else { 
            return 
        }
        
        do {
            if !audioEngine.isRunning {
                print("ðŸŽµ [AIVoice] Starting audio engine...")
                audioEngine.prepare()
                try audioEngine.start()
                isRecording = true
                print("ðŸŽ™ï¸ [AIVoice] âœ… Audio engine started - isRecording: \(isRecording)")
            }
        } catch {
            print("âŒ [AIVoice] Failed to start audio engine: \(error)")
        }
    }
    
    private func stopAudioEngine() {
        guard let audioEngine = audioEngine else { return }
        
        if audioEngine.isRunning {
            audioEngine.stop()
            isRecording = false
            audioChunkIndex = 0
            
            Task { @MainActor in
                isListening = false
            }
            
            print("â¹ï¸ [AIVoice] Audio engine stopped")
        }
    }
    
    deinit {
        cleanup()
        inputNode?.removeTap(onBus: 0)
        print("ðŸ§¹ [AIVoice] Audio service deallocated")
    }
    
    // MARK: - ðŸ”§ ä¿®å¤åŽçš„ç»Ÿä¸€éŸ³é¢‘æ’­æ”¾ç³»ç»Ÿ
    
    private func stopAllAudio() {
        audioPlaybackQueue.async {
            // åœæ­¢å½“å‰æ’­æ”¾
            self.currentAudioPlayer?.stop()
            self.currentAudioPlayer = nil
            
            // æ¸…ç©ºé˜Ÿåˆ—
            self.audioQueue.removeAll()
            self.audioAccumulator = Data()
            
            DispatchQueue.main.async {
                self.isAISpeaking = false
            }
            
            self.isPlayingAudio = false
            print("ðŸ”‡ [AIVoice] All audio playback stopped and cleared")
        }
    }
    
    private func addAudioChunk(_ base64AudioData: String) {
        guard let audioData = Data(base64Encoded: base64AudioData) else {
            print("âŒ [AIVoice] Failed to decode audio chunk")
            return
        }
        
        audioPlaybackQueue.async {
            // ç´¯ç§¯éŸ³é¢‘æ•°æ®ï¼ˆGPT-4oå‘é€çš„æ˜¯PCM16ç‰‡æ®µï¼‰
            let previousSize = self.audioAccumulator.count
            self.audioAccumulator.append(audioData)
            print("ðŸŽµ [AIVoice] Audio chunk accumulated: +\(audioData.count) bytes, total: \(previousSize) â†’ \(self.audioAccumulator.count) bytes")
        }
    }
    
    private func finalizeAndPlayAudio() {
        audioPlaybackQueue.async {
            guard !self.audioAccumulator.isEmpty else {
                print("ðŸ”‡ [AIVoice] No accumulated audio to play")
                return
            }
            
            print("ðŸ”Š [AIVoice] Finalizing and playing complete audio response: \(self.audioAccumulator.count) bytes")
            
            // è½¬æ¢PCM16æ•°æ®ä¸ºWAVæ ¼å¼ç”¨äºŽæ’­æ”¾
            let wavData = self.convertPCM16ToWAV(self.audioAccumulator)
            
            // æ·»åŠ åˆ°æ’­æ”¾é˜Ÿåˆ—
            self.audioQueue.append(wavData)
            
            // å¼€å§‹æ’­æ”¾é˜Ÿåˆ—ï¼ˆå¦‚æžœå½“å‰æ²¡æœ‰åœ¨æ’­æ”¾ï¼‰
            if !self.isPlayingAudio {
                self.playNextAudioInQueue()
            }
            
            // æ¸…ç©ºç´¯ç§¯å™¨ï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªå“åº”
            self.audioAccumulator = Data()
        }
    }
    
    private func playNextAudioInQueue() {
        audioPlaybackQueue.async {
            guard !self.isPlayingAudio, !self.audioQueue.isEmpty else {
                return
            }
            
            let audioData = self.audioQueue.removeFirst()
            self.isPlayingAudio = true
            
            DispatchQueue.main.async {
                self.isAISpeaking = true
            }
            
            do {
                // åœæ­¢ä¹‹å‰çš„æ’­æ”¾å™¨
                self.currentAudioPlayer?.stop()
                
                // åˆ›å»ºæ–°çš„æ’­æ”¾å™¨
                self.currentAudioPlayer = try AVAudioPlayer(data: audioData)
                self.currentAudioPlayer?.delegate = self
                
                // å¼€å§‹æ’­æ”¾
                let success = self.currentAudioPlayer?.play() ?? false
                print("ðŸ”Š [AIVoice] \(success ? "âœ… Started" : "âŒ Failed to start") playing audio: \(audioData.count) bytes")
                
                if !success {
                    self.audioPlaybackFinished()
                }
                
            } catch {
                print("âŒ [AIVoice] Failed to create audio player: \(error)")
                self.audioPlaybackFinished()
            }
        }
    }
    
    private func audioPlaybackFinished() {
        audioPlaybackQueue.async {
            self.isPlayingAudio = false
            
            DispatchQueue.main.async {
                if self.audioQueue.isEmpty {
                    self.isAISpeaking = false
                }
            }
            
            // ç»§ç»­æ’­æ”¾é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªéŸ³é¢‘
            if !self.audioQueue.isEmpty {
                self.playNextAudioInQueue()
            }
            
            print("ðŸ”Š [AIVoice] Audio playback finished, queue remaining: \(self.audioQueue.count)")
        }
    }
    
    private func convertPCM16ToWAV(_ pcmData: Data) -> Data {
        // WAVæ–‡ä»¶å¤´ä¿¡æ¯ (24kHz, 16-bit, mono)
        let sampleRate: UInt32 = 24000
        let channels: UInt16 = 1
        let bitsPerSample: UInt16 = 16
        let byteRate = sampleRate * UInt32(channels) * UInt32(bitsPerSample) / 8
        let blockAlign = channels * bitsPerSample / 8
        let dataSize = UInt32(pcmData.count)
        let fileSize = 36 + dataSize
        
        var wavData = Data()
        
        // RIFFå¤´
        wavData.append("RIFF".data(using: .ascii)!)
        wavData.append(withUnsafeBytes(of: fileSize.littleEndian) { Data($0) })
        wavData.append("WAVE".data(using: .ascii)!)
        
        // fmtå—
        wavData.append("fmt ".data(using: .ascii)!)
        wavData.append(withUnsafeBytes(of: UInt32(16).littleEndian) { Data($0) }) // fmtå—å¤§å°
        wavData.append(withUnsafeBytes(of: UInt16(1).littleEndian) { Data($0) })  // PCMæ ¼å¼
        wavData.append(withUnsafeBytes(of: channels.littleEndian) { Data($0) })
        wavData.append(withUnsafeBytes(of: sampleRate.littleEndian) { Data($0) })
        wavData.append(withUnsafeBytes(of: byteRate.littleEndian) { Data($0) })
        wavData.append(withUnsafeBytes(of: blockAlign.littleEndian) { Data($0) })
        wavData.append(withUnsafeBytes(of: bitsPerSample.littleEndian) { Data($0) })
        
        // dataå—
        wavData.append("data".data(using: .ascii)!)
        wavData.append(withUnsafeBytes(of: dataSize.littleEndian) { Data($0) })
        wavData.append(pcmData)
        
        return wavData
    }
    
    // MARK: - Helper æ–¹æ³•
    
    private func generateSessionId() -> String {
        return "ai_waiting_\(UUID().uuidString)_\(Date().timeIntervalSince1970)"
    }
    
    private func handleMatchFound(_ message: [String: Any]) {
        print("ðŸŽ¯ [AIVoice] Processing match found message")
        
        guard let matchId = message["match_id"] as? String,
              let sessionId = message["session_id"] as? String,
              let roomId = message["room_id"] as? String,
              let livekitToken = message["livekit_token"] as? String else {
            print("âŒ [AIVoice] Invalid match data received")
            return
        }
        
        // Parse participants
        let participantsData = message["participants"] as? [[String: Any]] ?? []
        let participants = participantsData.compactMap { data -> MatchParticipant? in
            guard let userId = data["user_id"] as? String,
                  let displayName = data["display_name"] as? String,
                  let isCurrentUser = data["is_current_user"] as? Bool else {
                return nil
            }
            return MatchParticipant(userId: userId, displayName: displayName, isCurrentUser: isCurrentUser)
        }
        
        let topics = message["topics"] as? [String] ?? []
        let hashtags = message["hashtags"] as? [String] ?? []
        
        let liveMatchData = LiveMatchData(
            matchId: matchId,
            sessionId: sessionId,
            roomId: roomId,
            livekitToken: livekitToken,
            participants: participants,
            topics: topics,
            hashtags: hashtags
        )
        
        print("âœ… [AIVoice] Match data processed successfully")
        print("   ðŸ·ï¸ Topics: \(topics)")
        print("   #ï¸âƒ£ Hashtags: \(hashtags)")
        print("   ðŸ‘¥ Participants: \(participants.count)")
        
        // Update on main thread
        DispatchQueue.main.async {
            self.matchFound = liveMatchData
        }
    }
    
    // MARK: - WebSocketDelegate
    
    func webSocketDidConnect(_ service: WebSocketService) {
        print("âœ… [AIVoice] WebSocket connected to GPT-4o Realtime")
        
        DispatchQueue.main.async {
            self.isConnected = true
        }
        
        // å‘é€è®¤è¯æ¶ˆæ¯
        let authMessage: [String: Any] = [
            "type": "auth",
            "token": authToken ?? ""
        ]
        
        service.send(authMessage)
        print("ðŸ“¤ [AIVoice] Sent authentication")
    }
    
    func webSocketDidDisconnect(_ service: WebSocketService) {
        print("âŒ [AIVoice] WebSocket disconnected")
        
        DispatchQueue.main.async {
            self.isConnected = false
            self.isListening = false
        }
        
        stopAllAudio()
    }
    
    func webSocket(_ service: WebSocketService, didReceiveMessage message: [String: Any]) {
        print("ðŸ“¥ [AIVoice] Received message: \(message["type"] as? String ?? "unknown")")
        
        guard let type = message["type"] as? String else { return }
        
        switch type {
        case "authenticated":
            print("âœ… [AIVoice] Authenticated with backend")
            if !isAuthenticated {
                sendStartSession()
                isAuthenticated = true
            }
            
        case "session_started":
            print("âœ… [AIVoice] Session started")
            sessionStarted = true
            Task {
                await startListening()
            }
            
        case "stt_chunk":
            // ä¸æ˜¾ç¤ºéƒ¨åˆ†è½¬å†™ï¼Œé¿å…UIé—ªçƒ
            break
            
        case "stt_done":
            print("âœ… [AIVoice] Complete transcription received")
            if let text = message["text"] as? String {
                print("ðŸ“âœ… [AIVoice] You said: '\(text)'")
                DispatchQueue.main.async {
                    self.currentResponse = "" // æ¸…ç©ºï¼Œå‡†å¤‡æŽ¥æ”¶AIå›žå¤
                }
            }
            
        case "speech_started":
            print("ðŸŽ¤ [AIVoice] User speech started")
            
        case "speech_stopped":
            print("ðŸ”‡ [AIVoice] User speech stopped")
            
        case "ai_response_started":
            print("ðŸ¤– [AIVoice] AI response started")
            stopAllAudio() // åœæ­¢ä¹‹å‰çš„éŸ³é¢‘ï¼Œå¼€å§‹æ–°å“åº”
            
        case "response.text.delta":
            print("ðŸ“ [AIVoice] Text delta received")
            if let textDelta = message["delta"] as? String {
                DispatchQueue.main.async {
                    self.currentResponse += textDelta
                }
            }
            
        case "response.audio.delta":
            print("ðŸ”Š [AIVoice] Audio delta received (using this, ignoring audio_chunk to prevent duplicates)")
            if let audioData = message["delta"] as? String {
                addAudioChunk(audioData)
            }
            
        case "response.done":
            print("âœ… [AIVoice] AI response completed")
            finalizeAndPlayAudio() // å®Œæˆç´¯ç§¯å¹¶æ’­æ”¾
            
        case "match_found":
            print("ðŸŽ¯ [AIVoice] Match found!")
            handleMatchFound(message)
            
        case "error":
            print("âŒ [AIVoice] WebSocket error: \(message["message"] as? String ?? "unknown")")
            
        default:
            print("â“ [AIVoice] Unknown message type: \(type)")
        }
    }
    
    func webSocket(_ service: WebSocketService, didEncounterError error: Error) {
        print("âŒ [AIVoice] WebSocket error: \(error)")
        
        DispatchQueue.main.async {
            self.isConnected = false
            self.isListening = false
        }
    }
}

// MARK: - AVAudioPlayerDelegate
extension AIVoiceService {
    func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
        print("ðŸ”Š [AIVoice] Audio finished playing successfully: \(flag)")
        audioPlaybackFinished()
    }
    
    func audioPlayerDecodeErrorDidOccur(_ player: AVAudioPlayer, error: Error?) {
        print("âŒ [AIVoice] Audio decode error: \(error?.localizedDescription ?? "unknown")")
        audioPlaybackFinished()
    }
}

struct VoicePinchedCircle: View {
    var level: CGFloat
    private let baseDiameter: CGFloat = 34
    private let growth: CGFloat = 38

    var body: some View {
        Capsule()
            .fill(Color(white: 0.93))
            .frame(width: baseDiameter,
                   height: baseDiameter + level * growth)
            .shadow(radius: 2)
    }
}

// ä¿æŒå‘åŽå…¼å®¹çš„é¢„è§ˆ
struct UserVoiceInput: View {
    var body: some View {
        NavigationView {
            UserVoiceTopicMatchingView(matchResult: MatchResult(
                transcription: "I want to talk about AI",
                topics: ["Artificial Intelligence", "Technology"],
                hashtags: ["#AI", "#Tech"],
                matchId: "preview_match",
                sessionId: "preview_session",
                confidence: 0.8,
                waitTime: 30
            ))
        }
    }
}

#Preview {
    UserVoiceInput()
}

// MARK: - Extensions
extension AVAudioPCMBuffer {
    func toData() -> Data? {
        let audioBuffer = audioBufferList.pointee.mBuffers
        let data = Data(bytes: audioBuffer.mData!, count: Int(audioBuffer.mDataByteSize))
        return data
    }
}
