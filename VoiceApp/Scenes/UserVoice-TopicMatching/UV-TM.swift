//
//  UV-TM.swift
//  VoiceApp
//
//  Created by Amedeo Ercole on 7/18/25.
//
import SwiftUI
import Combine
import AVFoundation

private enum CaptureState { case idle, listening, result }

// ç­‰å€™å®¤è§†å›¾ - ä¸Ž AI è¿›è¡Œè¯­éŸ³å¯¹è¯ï¼ŒåŒæ—¶åœ¨åŽå°åŒ¹é…ç”¨æˆ·
struct UserVoiceTopicMatchingView: View {
    let matchResult: MatchResult
    @Environment(\.dismiss) private var dismiss
    
    @State private var captureState: CaptureState = .idle
    @State private var levels: [CGFloat] = Array(repeating: 0.1, count: 3)
    @State private var currentText: String = ""
    @State private var isConnectedToAI = false
    @StateObject private var aiVoiceService = AIVoiceService()

    // demo timer â€“ swap with AVAudioEngine later
    private let timer = Timer.publish(every: 0.12, on: .main, in: .common).autoconnect()

    var body: some View {
        ZStack {
            Color.black.ignoresSafeArea()

            sidebar
            topRightCluster
            bubblesIfListening
            pullHandle
            micIdleButton
            bottomResultButtons
        }
        .onReceive(timer) { _ in
            if captureState == .listening {
                levels = levels.map { _ in .random(in: 0.05...1) }
            }
        }
        .onAppear {
            setupWaitingRoom()
        }
        .onReceive(aiVoiceService.$currentResponse) { response in
            if !response.isEmpty {
                print("ðŸŽ¨ [WaitingRoom] Received AI response, updating UI: \(String(response.prefix(50)))...")
                captureState = .result
                currentText = response
                
                // è¾ƒé•¿æ—¶é—´æ˜¾ç¤ºå“åº”åŽå›žåˆ° idle çŠ¶æ€
                DispatchQueue.main.asyncAfter(deadline: .now() + 8) {
                    captureState = .idle
                    currentText = "What else would you like to discuss about \(matchResult.topics.first ?? "this topic")? Keep talking while I find others to join!"
                }
            }
        }
        .onReceive(aiVoiceService.$isListening) { isListening in
            if isListening && captureState == .idle {
                captureState = .listening
                currentText = "ðŸŽ¤ Now listening continuously..."
            }
        }
        .onReceive(aiVoiceService.$isAISpeaking) { isSpeaking in
            if isSpeaking {
                captureState = .result
                currentText = "ðŸ”Š AI is speaking..."
            }
        }
        .onReceive(aiVoiceService.$isMuted) { isMuted in
            if captureState == .listening {
                currentText = isMuted ? "ðŸ”‡ Muted - Tap mic to unmute" : "ðŸŽ¤ Listening continuously..."
            }
        }
        .onReceive(aiVoiceService.$isConnected) { connected in
            if connected {
                print("ðŸ”— [WaitingRoom] Connected to AI service")
            }
        }
        .navigationBarHidden(true)
    }

    
    private var sidebar: some View {
        VStack {
            // è¿”å›žæŒ‰é’®
            Button(action: {
                print("ðŸ”™ [WaitingRoom] User tapped back button")
                VoiceMatchingService.shared.resetNavigation()
                dismiss()
            }) {
                Image(systemName: "arrow.left")
                    .font(.title2)
                    .foregroundColor(.white)
                    .frame(width: 40, height: 40)
                    .background(Color.black.opacity(0.3))
                    .clipShape(Circle())
            }
            .padding(.bottom, 20)
            
            // åŽŸæ¥çš„ sidebar å›¾æ ‡
        Image("sidebar")
            .resizable()
            .aspectRatio(contentMode: .fit)
            .frame(width: 50, height: 204)
            .shadow(color: .white, radius: 12)
        }
            .padding(.top, -120)
            .padding(.leading, 4)
            .frame(maxWidth: .infinity,
                   maxHeight: .infinity,
                   alignment: .topLeading)
    }

    
    private var topRightCluster: some View {
        VStack(alignment: .trailing, spacing: 24) {
            Image("orb")
                .resizable().aspectRatio(contentMode: .fit)
                .frame(width: 170, height: 170)
                .overlay(
                    LinearGradient(
                        gradient: Gradient(stops: [
                            .init(color: .clear,                   location: 0),
                            .init(color: Color.black.opacity(0.4), location: 0.3),
                            .init(color: Color.black.opacity(0.7), location: 1)
                        ]),
                        startPoint: .top, endPoint: .bottom)
                )
                .overlay(
                    RadialGradient(
                        gradient: Gradient(colors: [.clear, Color.black.opacity(0.5)]),
                        center: .center, startRadius: 0, endRadius: 85)
                )

            if captureState == .idle {
                Text(currentText)
                    .font(.custom("Rajdhani", size: 28))  // ç¨å¾®å‡å°å­—ä½“
                    .foregroundColor(.white)
                    .multilineTextAlignment(.leading)
                    .lineLimit(nil)  // å…è®¸æ— é™è¡Œ
                    .frame(maxWidth: 320, alignment: .leading)  // å¢žåŠ æœ€å¤§å®½åº¦
                    .padding(.horizontal, 20)  // æ·»åŠ æ°´å¹³è¾¹è·
                    .offset(x: -40)
            } else if captureState == .listening {
                Text(aiVoiceService.isMuted ? "ðŸ”‡ Muted - Tap mic to unmute" : "ðŸŽ¤ Listening continuously...")
                    .font(.custom("Rajdhani", size: 32))
                    .foregroundColor(aiVoiceService.isMuted ? .red : .green)
                    .multilineTextAlignment(.leading)
                    .frame(maxWidth: 300, alignment: .leading)
                    .offset(x: -40)
            } else if captureState == .result {
                if aiVoiceService.isAISpeaking {
                    Text("ðŸ”Š AI is speaking...")
                        .font(.custom("Rajdhani", size: 32))
                        .foregroundColor(.blue)
                        .multilineTextAlignment(.leading)
                        .frame(maxWidth: 300, alignment: .leading)
                        .offset(x: -40)
                } else {
                    ScrollView {
                        Text(currentText)
                            .font(.custom("Rajdhani", size: 26))  // ç¨å¾®å‡å°å­—ä½“ä»¥é€‚åº”æ›´å¤šå†…å®¹
                .foregroundColor(.white)
                .multilineTextAlignment(.leading)
                            .lineLimit(nil)
                .frame(maxWidth: 320, alignment: .leading)
                            .padding(.horizontal, 20)
                    }
                    .frame(maxHeight: 200)  // é™åˆ¶æ»šåŠ¨åŒºåŸŸé«˜åº¦
                .offset(x: -40)
                }
            }
        }
        .padding(.top, 8)
        .padding(.trailing, 8)
        .frame(maxWidth: .infinity,
               maxHeight: .infinity,
               alignment: .topTrailing)
    }

    
    private var bubblesIfListening: some View {
        VStack {
            Spacer()
            if captureState == .listening {
                HStack(spacing: 24) {
                    ForEach(levels.indices, id: \.self) { idx in
                        VoicePinchedCircle(level: levels[idx])
                            .animation(.easeInOut(duration: 0.1), value: levels[idx])
                    }
                }
                .frame(height: 100)
                .padding(.bottom, 135)
            }
        }
    }

    
    private var pullHandle: some View {
        Image("pullhandle")
            .resizable()
            .aspectRatio(contentMode: .fit)
            .frame(width: 120, height: 50)
            .shadow(color: Color.white.opacity(0.25), radius: 2)
            .offset(y: 38)
            .frame(maxWidth: .infinity,
                   maxHeight: .infinity,
                   alignment: .bottom)
    }

   
    private var micIdleButton: some View {
        Group {
            if captureState == .idle || captureState == .listening {
                Button(action: {
                    if aiVoiceService.isConnected {
                        aiVoiceService.toggleMute()
                    }
                }) {
                    Image(systemName: aiVoiceService.isMuted ? "mic.slash" : "mic")
                    .resizable()
                    .frame(width: 64, height: 64)
                        .foregroundColor(aiVoiceService.isMuted ? .red : .white)
                    .shadow(radius: 4)
                }
                    .frame(maxWidth: .infinity,
                           maxHeight: .infinity,
                           alignment: .bottom)
                    .padding(.bottom, 120)
            }
        }
    }

    
    private var bottomResultButtons: some View {
        Group {
            if captureState == .result && !aiVoiceService.isAISpeaking {
                HStack(spacing: 24) {
                    Button(action: {
                        aiVoiceService.toggleMute()
                    }) {
                        Image(systemName: aiVoiceService.isMuted ? "mic.slash" : "mic")
                        .resizable()
                        .frame(width: 36, height: 36)
                            .foregroundColor(aiVoiceService.isMuted ? .red : .white)
                        .shadow(radius: 2)
                    }
                       
                    Button("Back to Chat") {
                        captureState = .listening
                        currentText = aiVoiceService.isMuted ? "ðŸ”‡ Muted - Tap mic to unmute" : "ðŸŽ¤ Listening continuously..."
                    }
                    .font(.headline)
                    .foregroundColor(.white)
                    .padding(.vertical, 14)
                    .padding(.horizontal, 28)
                    .background(.ultraThinMaterial)
                    .clipShape(Capsule())
                    .shadow(radius: 4)
                }
                .padding(.bottom, 85)
                .frame(maxWidth: .infinity,
                       maxHeight: .infinity,
                       alignment: .bottom)
            }
        }
    }

    // MARK: - AI è¯­éŸ³å¯¹è¯é€»è¾‘
    
    private func setupWaitingRoom() {
        print("ðŸ  [WaitingRoom] Setting up waiting room for topics: \(matchResult.topics)")
        
        // åˆå§‹åŒ– AI å¯¹è¯
        Task {
            await aiVoiceService.initializeAIConversation(with: matchResult)
        }
        
        // è®¾ç½®åˆå§‹æ–‡æœ¬
        let topicList = matchResult.topics.joined(separator: "\nâ€¢ ")
        currentText = "ðŸŽ¯ Great! I found these topics:\n\nâ€¢ \(topicList)\n\nðŸ¤– I'm your AI conversation partner!\n\nðŸŽ™ï¸ I'm listening - just start talking!"
    }
}

// MARK: - AI è¯­éŸ³æœåŠ¡ - GPT-4o Realtime WebSocket
class AIVoiceService: NSObject, ObservableObject, WebSocketDelegate, AVAudioPlayerDelegate {
    @Published var isConnected = false
    @Published var isListening = false
    @Published var isMuted = false
    @Published var currentResponse = ""
    @Published var isAISpeaking = false
    
    private var matchContext: MatchResult?
    private var conversationContext: String = ""
    private var webSocketService: WebSocketService?
    private var authToken: String?
    private var isAuthenticated = false
    private var sessionStarted = false
    
    // éŸ³é¢‘ç›¸å…³
    private var audioEngine: AVAudioEngine?
    private var inputNode: AVAudioInputNode?
    private var audioPlayer: AVAudioPlayer?
    private var isRecording = false
    private var silenceTimer: Timer?
    private var lastAudioTime: Date = Date()
    private var hasDetectedSpeech = false
    
    override init() {
        // èŽ·å–è®¤è¯ä»¤ç‰Œ
        authToken = AuthService.shared.firebaseToken
        super.init()
        setupAudioSession()
        setupAudioEngine()
    }
    
    private func setupAudioSession() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.defaultToSpeaker, .allowBluetooth])
            
            // è®¾ç½®é¦–é€‰é‡‡æ ·çŽ‡ä¸º16kHz
            try audioSession.setPreferredSampleRate(16000)
            try audioSession.setActive(true)
            
            print("âœ… [AIVoice] Audio session configured for 16kHz voice chat")
        } catch {
            print("âŒ [AIVoice] Audio session setup failed: \(error)")
        }
    }
    
    private func setupAudioEngine() {
        audioEngine = AVAudioEngine()
        inputNode = audioEngine?.inputNode
        
        guard let audioEngine = audioEngine,
              let inputNode = inputNode else {
            print("âŒ [AIVoice] Failed to setup audio engine")
            return
        }
        
        // ä½¿ç”¨ç¡¬ä»¶çš„å®žé™…è¾“å…¥æ ¼å¼ï¼Œé¿å…æ ¼å¼ä¸åŒ¹é…
        let inputFormat = inputNode.inputFormat(forBus: 0)
        print("ðŸŽ™ï¸ [AIVoice] Hardware input format: \(inputFormat)")
        
        // åˆ›å»ºç›®æ ‡æ ¼å¼ (16kHz, Int16, mono)
        guard let targetFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, 
                                             sampleRate: 16000, 
                                             channels: 1, 
                                             interleaved: false) else {
            print("âŒ [AIVoice] Failed to create target audio format")
            return
        }
        
        // å®‰è£…tapä½¿ç”¨ç¡¬ä»¶çš„åŽŸç”Ÿæ ¼å¼
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { [weak self] buffer, time in
            self?.processAudioBuffer(buffer, originalFormat: inputFormat, targetFormat: targetFormat)
        }
        
        print("âœ… [AIVoice] Audio engine configured for streaming")
    }
    
    func initializeAIConversation(with matchResult: MatchResult) async {
        print("ðŸ¤– [AIVoice] Initializing AI conversation for session: \(generateSessionId())")
        
        self.matchContext = matchResult
        
        // è®¾ç½®å¯¹è¯ä¸Šä¸‹æ–‡ - AIåº”è¯¥æ˜¯èŠå¤©ä¼™ä¼´ï¼Œä¸æ˜¯åŒ¹é…ç®—æ³•
        conversationContext = """
        You are a friendly AI conversation partner in a voice chat app. The user wants to discuss these topics: \(matchResult.topics.joined(separator: ", ")).
        
        Their original message was: "\(matchResult.transcription)"
        
        Key guidelines:
        - You are NOT a matching algorithm or service
        - You are a conversation partner who enjoys discussing these topics
        - Keep responses conversational and engaging
        - Ask follow-up questions to keep the conversation flowing
        - Use natural, spoken language (this is voice chat)
        - Don't mention "finding matches" or "waiting for others"
        - Focus on having an interesting discussion about the topics
        
        Hashtags for context: \(matchResult.hashtags.joined(separator: ", "))
        """
        
        print("ðŸ§  [AIVoice] AI conversation context set:")
        print("   ðŸ“ Transcription: \(matchResult.transcription)")
        print("   ðŸ·ï¸ Topics: \(matchResult.topics)")
        print("   #ï¸âƒ£ Hashtags: \(matchResult.hashtags)")
        
        // è¿žæŽ¥åˆ° WebSocket
        print("ðŸ”Œ [AIVoice] Connecting to GPT-4o Realtime API...")
        print("ðŸŽ¯ [AIVoice] Will send conversation context about: \(matchResult.topics)")
        
        await connectToRealtimeAPI()
        
        print("âœ… [AIVoice] AI conversation initialized with topic context")
    }
    
    private func connectToRealtimeAPI() async {
        guard let token = authToken else {
            print("âŒ [AIVoice] No auth token available")
            return
        }
        
        print("ðŸ”Œ [AIVoice] Connecting to GPT-4o Audio Stream API...")
        print("ðŸŽ¯ [AIVoice] Will send conversation context about: \(matchContext?.topics ?? [])")
        
        await MainActor.run {
            // åˆ›å»º WebSocket æœåŠ¡
            webSocketService = WebSocketService()
            webSocketService?.delegate = self
            
            // è¿žæŽ¥åˆ°æ–°çš„éŸ³é¢‘æµç«¯ç‚¹
            webSocketService?.connect(to: APIConfig.WebSocket.aiAudioStream, with: token)
        }
    }
    
    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer, originalFormat: AVAudioFormat, targetFormat: AVAudioFormat) {
        guard sessionStarted && !isMuted && isRecording else { 
            // print("ðŸ”‡ [AIVoice] Skipping audio - sessionStarted: \(sessionStarted), muted: \(isMuted), recording: \(isRecording)")
            return 
        }
        
        // æ£€æµ‹éŸ³é¢‘æ´»åŠ¨ (ç®€å•çš„èƒ½é‡æ£€æµ‹)
        let audioLevel = calculateAudioLevel(buffer)
        let speechThreshold: Float = 0.01  // å¯è°ƒæ•´çš„é˜ˆå€¼
        let isSpeechDetected = audioLevel > speechThreshold
        
        if isSpeechDetected {
            hasDetectedSpeech = true
            lastAudioTime = Date()
            // å–æ¶ˆé™éŸ³è®¡æ—¶å™¨
            silenceTimer?.invalidate()
            silenceTimer = nil
        } else if hasDetectedSpeech {
            // å¦‚æžœä¹‹å‰æ£€æµ‹åˆ°è¯­éŸ³ï¼ŒçŽ°åœ¨æ˜¯é™éŸ³ï¼Œå¼€å§‹è®¡æ—¶
            if silenceTimer == nil {
                print("ðŸ”‡ [AIVoice] Silence detected, starting timer...")
                silenceTimer = Timer.scheduledTimer(withTimeInterval: 2.0, repeats: false) { _ in
                    self.triggerUtteranceEnd()
                }
            }
        }
        
        print("ðŸŽ¤ [AIVoice] Processing audio buffer - frames: \(buffer.frameLength), level: \(audioLevel), speech: \(isSpeechDetected)")
        
        // åˆ›å»ºéŸ³é¢‘è½¬æ¢å™¨
        guard let converter = AVAudioConverter(from: originalFormat, to: targetFormat) else {
            print("âŒ [AIVoice] Failed to create audio converter")
            return
        }
        
        // åˆ›å»ºè¾“å‡ºç¼“å†²åŒº
        let outputFrameCapacity = AVAudioFrameCount(Double(buffer.frameLength) * targetFormat.sampleRate / originalFormat.sampleRate)
        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: outputFrameCapacity) else {
            print("âŒ [AIVoice] Failed to create output buffer")
            return
        }
        
        // æ‰§è¡Œæ ¼å¼è½¬æ¢
        var error: NSError?
        _ = converter.convert(to: outputBuffer, error: &error) { _, outStatus in
            outStatus.pointee = .haveData
            return buffer
        }
        
        if let error = error {
            print("âŒ [AIVoice] Audio conversion failed: \(error)")
            return
        }
        
        // è½¬æ¢ä¸ºData
        guard let audioData = outputBuffer.toData() else {
            print("âŒ [AIVoice] Failed to convert converted buffer to data")
            return
        }
        
        print("ðŸŽµ [AIVoice] Audio converted successfully: \(audioData.count) bytes (from \(buffer.frameLength) frames)")
        
        // ç¼–ç ä¸ºbase64å¹¶å‘é€
        let base64Audio = audioData.base64EncodedString()
        
        let audioMessage: [String: Any] = [
            "type": "audio_chunk",
            "audio_data": base64Audio,
            "language": "en-US",
            "timestamp": Date().timeIntervalSince1970
        ]
        
        webSocketService?.send(audioMessage)
        print("ðŸ“¤ [AIVoice] Sent audio chunk: \(audioData.count) bytes (\(base64Audio.count) base64 chars)")
    }
    
    private func calculateAudioLevel(_ buffer: AVAudioPCMBuffer) -> Float {
        guard let channelData = buffer.floatChannelData?[0] else { return 0.0 }
        
        var sum: Float = 0.0
        let frameCount = Int(buffer.frameLength)
        
        for i in 0..<frameCount {
            sum += abs(channelData[i])
        }
        
        return frameCount > 0 ? sum / Float(frameCount) : 0.0
    }
    
    private func triggerUtteranceEnd() {
        guard hasDetectedSpeech else { return }
        
        print("ðŸ”š [AIVoice] Triggering utterance end...")
        
        let utteranceEndMessage: [String: Any] = [
            "type": "utterance_end",
            "timestamp": Date().timeIntervalSince1970
        ]
        
        webSocketService?.send(utteranceEndMessage)
        
        // é‡ç½®çŠ¶æ€
        hasDetectedSpeech = false
        silenceTimer?.invalidate()
        silenceTimer = nil
        
        print("ðŸ”š [AIVoice] Utterance end sent, state reset")
    }
    
    private func sendStartSession() {
        let startSessionMessage: [String: Any] = [
            "type": "start_session",
            "user_context": [
                "topics": matchContext?.topics ?? [],
                "hashtags": matchContext?.hashtags ?? [],
                "transcription": matchContext?.transcription ?? "",
                "conversation_context": conversationContext
            ],
            "timestamp": Date().timeIntervalSince1970
        ]
        
        webSocketService?.send(startSessionMessage)
        print("ðŸ“¤ [AIVoice] Sent start session with topic context")
    }
    
    func startListening() async {
        guard sessionStarted && !isMuted else {
            print("âš ï¸ [AIVoice] Cannot start listening - session not started or muted")
            print("âš ï¸ [AIVoice] sessionStarted: \(sessionStarted), isMuted: \(isMuted)")
            return
        }
        
        print("ðŸŽ¤ [AIVoice] Starting continuous voice listening with audio engine")
        print("ðŸŽ¤ [AIVoice] Current state - sessionStarted: \(sessionStarted), isMuted: \(isMuted)")
        
        await MainActor.run {
            isListening = true
        }
        
        startAudioEngine()
    }
    
    func toggleMute() {
        isMuted.toggle()
        print("ðŸ”‡ [AIVoice] Audio input \(isMuted ? "muted" : "unmuted")")
        print("ðŸ”‡ [AIVoice] New state - isMuted: \(isMuted), isRecording: \(isRecording)")
        
        if isMuted {
            stopAudioEngine()
        } else if sessionStarted {
            startAudioEngine()
        }
    }
    
    private func startAudioEngine() {
        guard !isMuted, let audioEngine = audioEngine else { 
            print("âš ï¸ [AIVoice] Cannot start audio engine - muted: \(isMuted), engine exists: \(audioEngine != nil)")
            return 
        }
        
        do {
            // ç¡®ä¿å¼•æ“Žå·²å‡†å¤‡å¥½
            if !audioEngine.isRunning {
                print("ðŸŽµ [AIVoice] Preparing and starting audio engine...")
                audioEngine.prepare()
                try audioEngine.start()
                isRecording = true
                print("ðŸŽ™ï¸ [AIVoice] âœ… Audio engine started for streaming - isRecording: \(isRecording)")
            } else {
                print("ðŸŽ™ï¸ [AIVoice] Audio engine already running")
            }
        } catch {
            print("âŒ [AIVoice] Failed to start audio engine: \(error)")
        }
    }
    
    private func stopAudioEngine() {
        guard let audioEngine = audioEngine else { return }
        
        if audioEngine.isRunning {
            audioEngine.stop()
            isRecording = false
            
            // æ¸…ç†è¯­éŸ³æ£€æµ‹çŠ¶æ€
            silenceTimer?.invalidate()
            silenceTimer = nil
            hasDetectedSpeech = false
            
            Task { @MainActor in
                isListening = false
            }
            
            print("â¹ï¸ [AIVoice] Audio engine stopped, state cleared")
        }
    }
    
    deinit {
        // æ¸…ç†éŸ³é¢‘å¼•æ“Ž
        stopAudioEngine()
        inputNode?.removeTap(onBus: 0)
        silenceTimer?.invalidate()
        print("ðŸ§¹ [AIVoice] Audio service cleaned up")
    }
    
    // ç”ŸæˆåŸºäºŽè¯é¢˜çš„ AI å“åº”ï¼ˆå½“å‰æ˜¯æ¨¡æ‹Ÿï¼ŒçœŸå®žå®žçŽ°ä¼šæ¥è‡ª GPT-4oï¼‰
    func generateTopicBasedResponse() -> String {
        guard let context = matchContext else {
            return "Let's continue our conversation!"
        }
        
        let topic = context.topics.first ?? "this topic"
        let responses = [
            "That's fascinating! What specifically interests you about \(topic)?",
            "I'd love to hear more about your experience with \(topic).",
            "What got you started with \(topic)? Any interesting stories?",
            "Have you seen any recent developments in \(topic) that caught your attention?",
            "What aspects of \(topic) do you think others should know about?"
        ]
        
        return responses.randomElement() ?? "Tell me more about \(topic)!"
    }
    
    // MARK: - Helper æ–¹æ³•
    
    private func generateSessionId() -> String {
        return "ai_waiting_\(UUID().uuidString)_\(Date().timeIntervalSince1970)"
    }
    
    // MARK: - WebSocketDelegate
    
    func webSocketDidConnect(_ service: WebSocketService) {
        print("âœ… [AIVoice] WebSocket connected to GPT-4o Realtime")
        
        DispatchQueue.main.async {
            self.isConnected = true
        }
        
        // å‘é€è®¤è¯æ¶ˆæ¯
        let authMessage: [String: Any] = [
            "type": "auth",
            "token": authToken ?? ""
        ]
        
        service.send(authMessage)
        print("ðŸ“¤ [AIVoice] Sent authentication")
    }
    
    func webSocketDidDisconnect(_ service: WebSocketService) {
        print("âŒ [AIVoice] WebSocket disconnected")
        
        DispatchQueue.main.async {
            self.isConnected = false
            self.isListening = false
        }
    }
    
    func webSocket(_ service: WebSocketService, didReceiveMessage message: [String: Any]) {
        print("ðŸ“¥ [AIVoice] Received message: \(message["type"] as? String ?? "unknown")")
        
        guard let type = message["type"] as? String else { return }
        
        switch type {
        case "authenticated":
            print("âœ… [AIVoice] Authenticated with backend")
            // åªåœ¨ç¬¬ä¸€æ¬¡è®¤è¯æ—¶å¼€å§‹ä¼šè¯
            if !isAuthenticated {
                sendStartSession()
                isAuthenticated = true
            }
            
        case "session_started":
            print("âœ… [AIVoice] Session started")
            print("ðŸŽ¯ [AIVoice] Session message data: \(message)")
            sessionStarted = true
            
            // ä¼šè¯å¼€å§‹åŽè‡ªåŠ¨å¼€å§‹ç›‘å¬
            print("ðŸš€ [AIVoice] About to start listening...")
            Task {
                await startListening()
            }
            
        case "stt_chunk":
            print("ðŸ“ [AIVoice] Received STT chunk")
            if let text = message["text"] as? String {
                print("ðŸŽ¤ðŸ“ [AIVoice] Partial transcription: '\(text)'")
                print("ðŸŽ¤ðŸ“ [AIVoice] Confidence: \(message["confidence"] ?? "unknown")")
                // Could update UI with partial transcription
                DispatchQueue.main.async {
                    self.currentResponse = "ðŸŽ¤ Hearing: \(text)"
                }
            }
            
        case "stt_done":
            print("âœ… [AIVoice] Complete transcription received")
            if let text = message["text"] as? String {
                print("ðŸ“âœ… [AIVoice] COMPLETE UTTERANCE: '\(text)'")
                print("ðŸ“âœ… [AIVoice] Full message data: \(message)")
                // UI could show the complete transcription
                DispatchQueue.main.async {
                    self.currentResponse = "âœ… You said: \(text)"
                }
            }
            
        case "ai_response":
            print("ðŸ¤– [AIVoice] Received AI response")
            if let responseText = message["text"] as? String {
                print("ðŸ’¬ðŸ¤– [AIVoice] AI FULL RESPONSE: '\(responseText)'")
                print("ðŸ’¬ðŸ¤– [AIVoice] Response length: \(responseText.count) characters")
                print("ðŸ’¬ðŸ¤– [AIVoice] Full AI message data: \(message)")
                
                DispatchQueue.main.async {
                    self.currentResponse = responseText
                    self.isListening = false
                }
            }
            
        case "audio_response":
            print("ðŸ”Š [AIVoice] Received AI audio response")
            if let audioData = message["audio"] as? String {
                print("ðŸ”ŠðŸŽµ [AIVoice] Audio data length: \(audioData.count) base64 chars")
                print("ðŸ”ŠðŸŽµ [AIVoice] Audio format: \(message["format"] ?? "unknown")")
                playAudioResponse(audioData)
            }
            
        case "audio_chunk":
            print("ðŸŽµ [AIVoice] Received real-time audio chunk")
            if let audioDelta = message["audio_delta"] as? String {
                print("ðŸ”ŠðŸŽµ [AIVoice] Audio chunk length: \(audioDelta.count) base64 chars")
                print("ðŸ”ŠðŸŽµ [AIVoice] Audio format: \(message["format"] ?? "pcm16")")
                // For now, we'll accumulate chunks and play the complete response
                // Real-time streaming would require more complex audio handling
            }
            
        case "utterance_end":
            print("âœ… [AIVoice] Utterance ended")
            // å½“è¯­éŸ³æ´»åŠ¨ç»“æŸæ—¶ï¼Œé‡ç½®è¯­éŸ³æ£€æµ‹çŠ¶æ€
            hasDetectedSpeech = false
            lastAudioTime = Date()
            print("ðŸ”Š [AIVoice] New state - hasDetectedSpeech: \(hasDetectedSpeech), lastAudioTime: \(lastAudioTime)")
            
        case "error":
            print("âŒ [AIVoice] WebSocket error: \(message["message"] as? String ?? "unknown")")
            print("âŒ [AIVoice] Full error message: \(message)")
            
        default:
            print("â“ [AIVoice] Unknown message type: \(type)")
            print("â“ [AIVoice] Full unknown message: \(message)")
        }
    }
    
    private func playAudioResponse(_ audioData: String) {
        // è§£ç å¹¶æ’­æ”¾AIçš„éŸ³é¢‘å›žåº”
        guard let data = Data(base64Encoded: audioData) else {
            print("âŒ [AIVoice] Failed to decode audio data")
            return
        }
        
        do {
            audioPlayer = try AVAudioPlayer(data: data)
            audioPlayer?.delegate = self
            audioPlayer?.play()
            
            DispatchQueue.main.async {
                self.isAISpeaking = true
            }
            
            print("ðŸ”Š [AIVoice] Playing AI audio response")
        } catch {
            print("âŒ [AIVoice] Failed to play audio: \(error)")
        }
    }
    
    func webSocket(_ service: WebSocketService, didEncounterError error: Error) {
        print("âŒ [AIVoice] WebSocket error: \(error)")
        
        DispatchQueue.main.async {
            self.isConnected = false
            self.isListening = false
        }
    }
}

// MARK: - AVAudioPlayerDelegate
extension AIVoiceService {
    func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
        DispatchQueue.main.async {
            self.isAISpeaking = false
        }
        print("ðŸ”Š [AIVoice] AI finished speaking")
    }
}

struct VoicePinchedCircle: View {
    var level: CGFloat
    private let baseDiameter: CGFloat = 34
    private let growth: CGFloat = 38

    var body: some View {
        Capsule()
            .fill(Color(white: 0.93))
            .frame(width: baseDiameter,
                   height: baseDiameter + level * growth)
            .shadow(radius: 2)
    }
}

// ä¿æŒå‘åŽå…¼å®¹çš„é¢„è§ˆ
struct UserVoiceInput: View {
    var body: some View {
        UserVoiceTopicMatchingView(matchResult: MatchResult(
            transcription: "I want to talk about AI",
            topics: ["Artificial Intelligence", "Technology"],
            hashtags: ["#AI", "#Tech"],
            matchId: "preview_match",
            sessionId: "preview_session",
            confidence: 0.8,
            waitTime: 30
        ))
    }
}

#Preview {
    UserVoiceInput()
}

// MARK: - Extensions
extension AVAudioPCMBuffer {
    func toData() -> Data? {
        let audioBuffer = audioBufferList.pointee.mBuffers
        let data = Data(bytes: audioBuffer.mData!, count: Int(audioBuffer.mDataByteSize))
        return data
    }
}
