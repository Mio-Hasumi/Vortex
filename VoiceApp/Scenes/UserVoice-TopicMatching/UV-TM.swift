//
//  UV-TM.swift
//  VoiceApp
//
//  Created by Amedeo Ercole on 7/18/25.
//
import SwiftUI
import Combine
import AVFoundation

private enum CaptureState { case idle, listening, result }

// ç­‰å€™å®¤è§†å›¾ - ä¸ AI è¿›è¡Œè¯­éŸ³å¯¹è¯ï¼ŒåŒæ—¶åœ¨åå°åŒ¹é…ç”¨æˆ·
struct UserVoiceTopicMatchingView: View {
    let matchResult: MatchResult
    @Environment(\.dismiss) private var dismiss
    
    @State private var captureState: CaptureState = .idle
    @State private var levels: [CGFloat] = Array(repeating: 0.1, count: 3)
    @State private var currentText: String = ""
    @State private var isConnectedToAI = false
    @StateObject private var aiVoiceService = AIVoiceService()

    // demo timer â€“ swap with AVAudioEngine later
    private let timer = Timer.publish(every: 0.12, on: .main, in: .common).autoconnect()

    var body: some View {
        ZStack {
            Color.black.ignoresSafeArea()

            sidebar
            topRightCluster
            bubblesIfListening
            pullHandle
            micIdleButton
            bottomResultButtons
        }
        .onReceive(timer) { _ in
            if captureState == .listening {
                levels = levels.map { _ in .random(in: 0.05...1) }
            }
        }
        .onAppear {
            setupWaitingRoom()
        }
        .onReceive(aiVoiceService.$currentResponse) { response in
            if !response.isEmpty {
                print("ğŸ¨ [WaitingRoom] Received AI response, updating UI: \(String(response.prefix(50)))...")
                captureState = .result
                currentText = response
                
                // è¾ƒé•¿æ—¶é—´æ˜¾ç¤ºå“åº”åå›åˆ° idle çŠ¶æ€
                DispatchQueue.main.asyncAfter(deadline: .now() + 8) {
                    captureState = .idle
                    currentText = "What else would you like to discuss about \(matchResult.topics.first ?? "this topic")? Keep talking while I find others to join!"
                }
            }
        }
        .onReceive(aiVoiceService.$isListening) { isListening in
            if isListening && captureState == .idle {
                captureState = .listening
                currentText = "ğŸ¤ Now listening continuously..."
            }
        }
        .onReceive(aiVoiceService.$isAISpeaking) { isSpeaking in
            if isSpeaking {
                captureState = .result
                currentText = "ğŸ”Š AI is speaking..."
            }
        }
        .onReceive(aiVoiceService.$isMuted) { isMuted in
            if captureState == .listening {
                currentText = isMuted ? "ğŸ”‡ Muted - Tap mic to unmute" : "ğŸ¤ Listening continuously..."
            }
        }
        .onReceive(aiVoiceService.$isConnected) { connected in
            if connected {
                print("ğŸ”— [WaitingRoom] Connected to AI service")
            }
        }
        .navigationBarHidden(true)
    }

    
    private var sidebar: some View {
        VStack {
            // è¿”å›æŒ‰é’®
            Button(action: {
                print("ğŸ”™ [WaitingRoom] User tapped back button")
                VoiceMatchingService.shared.resetNavigation()
                dismiss()
            }) {
                Image(systemName: "arrow.left")
                    .font(.title2)
                    .foregroundColor(.white)
                    .frame(width: 40, height: 40)
                    .background(Color.black.opacity(0.3))
                    .clipShape(Circle())
            }
            .padding(.bottom, 20)
            
            // åŸæ¥çš„ sidebar å›¾æ ‡
        Image("sidebar")
            .resizable()
            .aspectRatio(contentMode: .fit)
            .frame(width: 50, height: 204)
            .shadow(color: .white, radius: 12)
        }
            .padding(.top, -120)
            .padding(.leading, 4)
            .frame(maxWidth: .infinity,
                   maxHeight: .infinity,
                   alignment: .topLeading)
    }

    
    private var topRightCluster: some View {
        VStack(alignment: .trailing, spacing: 24) {
            Image("orb")
                .resizable().aspectRatio(contentMode: .fit)
                .frame(width: 170, height: 170)
                .overlay(
                    LinearGradient(
                        gradient: Gradient(stops: [
                            .init(color: .clear,                   location: 0),
                            .init(color: Color.black.opacity(0.4), location: 0.3),
                            .init(color: Color.black.opacity(0.7), location: 1)
                        ]),
                        startPoint: .top, endPoint: .bottom)
                )
                .overlay(
                    RadialGradient(
                        gradient: Gradient(colors: [.clear, Color.black.opacity(0.5)]),
                        center: .center, startRadius: 0, endRadius: 85)
                )

            if captureState == .idle {
                Text(currentText)
                    .font(.custom("Rajdhani", size: 28))  // ç¨å¾®å‡å°å­—ä½“
                    .foregroundColor(.white)
                    .multilineTextAlignment(.leading)
                    .lineLimit(nil)  // å…è®¸æ— é™è¡Œ
                    .frame(maxWidth: 320, alignment: .leading)  // å¢åŠ æœ€å¤§å®½åº¦
                    .padding(.horizontal, 20)  // æ·»åŠ æ°´å¹³è¾¹è·
                    .offset(x: -40)
            } else if captureState == .listening {
                Text(aiVoiceService.isMuted ? "ğŸ”‡ Muted - Tap mic to unmute" : "ğŸ¤ Listening continuously...")
                    .font(.custom("Rajdhani", size: 32))
                    .foregroundColor(aiVoiceService.isMuted ? .red : .green)
                    .multilineTextAlignment(.leading)
                    .frame(maxWidth: 300, alignment: .leading)
                    .offset(x: -40)
            } else if captureState == .result {
                if aiVoiceService.isAISpeaking {
                    Text("ğŸ”Š AI is speaking...")
                        .font(.custom("Rajdhani", size: 32))
                        .foregroundColor(.blue)
                        .multilineTextAlignment(.leading)
                        .frame(maxWidth: 300, alignment: .leading)
                        .offset(x: -40)
                } else {
                    ScrollView {
                        Text(currentText)
                            .font(.custom("Rajdhani", size: 26))  // ç¨å¾®å‡å°å­—ä½“ä»¥é€‚åº”æ›´å¤šå†…å®¹
                .foregroundColor(.white)
                .multilineTextAlignment(.leading)
                            .lineLimit(nil)
                .frame(maxWidth: 320, alignment: .leading)
                            .padding(.horizontal, 20)
                    }
                    .frame(maxHeight: 200)  // é™åˆ¶æ»šåŠ¨åŒºåŸŸé«˜åº¦
                .offset(x: -40)
                }
            }
        }
        .padding(.top, 8)
        .padding(.trailing, 8)
        .frame(maxWidth: .infinity,
               maxHeight: .infinity,
               alignment: .topTrailing)
    }

    
    private var bubblesIfListening: some View {
        VStack {
            Spacer()
            if captureState == .listening {
                HStack(spacing: 24) {
                    ForEach(levels.indices, id: \.self) { idx in
                        VoicePinchedCircle(level: levels[idx])
                            .animation(.easeInOut(duration: 0.1), value: levels[idx])
                    }
                }
                .frame(height: 100)
                .padding(.bottom, 135)
            }
        }
    }

    
    private var pullHandle: some View {
        Image("pullhandle")
            .resizable()
            .aspectRatio(contentMode: .fit)
            .frame(width: 120, height: 50)
            .shadow(color: Color.white.opacity(0.25), radius: 2)
            .offset(y: 38)
            .frame(maxWidth: .infinity,
                   maxHeight: .infinity,
                   alignment: .bottom)
    }

   
    private var micIdleButton: some View {
        Group {
            if captureState == .idle || captureState == .listening {
                Button(action: {
                    if aiVoiceService.isConnected {
                        aiVoiceService.toggleMute()
                    }
                }) {
                    Image(systemName: aiVoiceService.isMuted ? "mic.slash" : "mic")
                    .resizable()
                    .frame(width: 64, height: 64)
                        .foregroundColor(aiVoiceService.isMuted ? .red : .white)
                    .shadow(radius: 4)
                }
                    .frame(maxWidth: .infinity,
                           maxHeight: .infinity,
                           alignment: .bottom)
                    .padding(.bottom, 120)
            }
        }
    }

    
    private var bottomResultButtons: some View {
        Group {
            if captureState == .result && !aiVoiceService.isAISpeaking {
                HStack(spacing: 24) {
                    Button(action: {
                        aiVoiceService.toggleMute()
                    }) {
                        Image(systemName: aiVoiceService.isMuted ? "mic.slash" : "mic")
                        .resizable()
                        .frame(width: 36, height: 36)
                            .foregroundColor(aiVoiceService.isMuted ? .red : .white)
                        .shadow(radius: 2)
                    }
                       
                    Button("Back to Chat") {
                        captureState = .listening
                        currentText = aiVoiceService.isMuted ? "ğŸ”‡ Muted - Tap mic to unmute" : "ğŸ¤ Listening continuously..."
                    }
                    .font(.headline)
                    .foregroundColor(.white)
                    .padding(.vertical, 14)
                    .padding(.horizontal, 28)
                    .background(.ultraThinMaterial)
                    .clipShape(Capsule())
                    .shadow(radius: 4)
                }
                .padding(.bottom, 85)
                .frame(maxWidth: .infinity,
                       maxHeight: .infinity,
                       alignment: .bottom)
            }
        }
    }

    // MARK: - AI è¯­éŸ³å¯¹è¯é€»è¾‘
    
    private func setupWaitingRoom() {
        print("ğŸ  [WaitingRoom] Setting up waiting room for topics: \(matchResult.topics)")
        
        // åˆå§‹åŒ– AI å¯¹è¯
        Task {
            await aiVoiceService.initializeAIConversation(with: matchResult)
        }
        
        // è®¾ç½®åˆå§‹æ–‡æœ¬
        let topicList = matchResult.topics.joined(separator: "\nâ€¢ ")
        currentText = "ğŸ¯ Great! I found these topics:\n\nâ€¢ \(topicList)\n\nğŸ¤– I'm your AI conversation partner!\n\nğŸ™ï¸ I'm listening - just start talking!"
    }
}

// MARK: - AI è¯­éŸ³æœåŠ¡ - GPT-4o Realtime WebSocket
class AIVoiceService: NSObject, ObservableObject, WebSocketDelegate, AVAudioPlayerDelegate {
    @Published var isConnected = false
    @Published var isListening = false
    @Published var isMuted = false
    @Published var currentResponse = ""
    @Published var isAISpeaking = false
    
    private var matchContext: MatchResult?
    private var conversationContext: String = ""
    private var webSocketService: WebSocketService?
    private var authToken: String?
    private var isAuthenticated = false
    private var sessionStarted = false
    
    // éŸ³é¢‘ç›¸å…³
    private var audioEngine: AVAudioEngine?
    private var inputNode: AVAudioInputNode?
    private var audioPlayer: AVAudioPlayer?
    private var isRecording = false
    private var silenceTimer: Timer?
    private var lastAudioTime: Date = Date()
    private var hasDetectedSpeech = false
    private var speechStartTime: Date?
    private var maxUtteranceDuration: TimeInterval = 8.0  // æœ€å¤§8ç§’utterance
    private var maxUtteranceTimer: Timer?
    private var forceUtteranceTimer: Timer?  // å¼ºåˆ¶è¶…æ—¶ï¼Œé˜²æ­¢æ°¸è¿œä¸å‘utterance_end
    private var audioStartTime: Date?
    private var audioChunkIndex: Int = 0  // è¿½è¸ªå‘é€çš„éŸ³é¢‘å—åºå·
    private var consecutiveSpeechFrames: Int = 0  // è¿ç»­æ£€æµ‹åˆ°è¯­éŸ³çš„å¸§æ•°
    private var consecutiveSilenceFrames: Int = 0  // è¿ç»­æ£€æµ‹åˆ°é™éŸ³çš„å¸§æ•°
    private let minSpeechFrames: Int = 3  // è‡³å°‘3å¸§è¿ç»­æ£€æµ‹åˆ°è¯­éŸ³æ‰ç®—è¯´è¯
    private let minSilenceFrames: Int = 5  // è‡³å°‘5å¸§è¿ç»­é™éŸ³æ‰ç®—åœæ­¢è¯´è¯
    
    override init() {
        // è·å–è®¤è¯ä»¤ç‰Œ
        authToken = AuthService.shared.firebaseToken
        super.init()
        setupAudioSession()
        setupAudioEngine()
    }
    
    private func setupAudioSession() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.defaultToSpeaker, .allowBluetooth])
            
            // è®¾ç½®é¦–é€‰é‡‡æ ·ç‡ä¸º16kHz
            try audioSession.setPreferredSampleRate(16000)
            try audioSession.setActive(true)
            
            print("âœ… [AIVoice] Audio session configured for 16kHz voice chat")
        } catch {
            print("âŒ [AIVoice] Audio session setup failed: \(error)")
        }
    }
    
    private func setupAudioEngine() {
        audioEngine = AVAudioEngine()
        inputNode = audioEngine?.inputNode
        
        guard let audioEngine = audioEngine,
              let inputNode = inputNode else {
            print("âŒ [AIVoice] Failed to setup audio engine")
            return
        }
        
        // ä½¿ç”¨ç¡¬ä»¶çš„å®é™…è¾“å…¥æ ¼å¼ï¼Œé¿å…æ ¼å¼ä¸åŒ¹é…
        let inputFormat = inputNode.inputFormat(forBus: 0)
        print("ğŸ™ï¸ [AIVoice] Hardware input format: \(inputFormat)")
        
        // åˆ›å»ºç›®æ ‡æ ¼å¼ (24kHz, Int16, mono) - åŒ¹é…OpenAI Realtime APIè¦æ±‚
        guard let targetFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, 
                                             sampleRate: 24000,  // OpenAIè¦æ±‚24kHz
                                             channels: 1, 
                                             interleaved: false) else {
            print("âŒ [AIVoice] Failed to create target audio format")
            return
        }
        
        print("ğŸµ [AIVoice] Target format for OpenAI: 24kHz, PCM16, mono")
        
        // å®‰è£…tapä½¿ç”¨ç¡¬ä»¶çš„åŸç”Ÿæ ¼å¼
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { [weak self] buffer, time in
            self?.processAudioBuffer(buffer, originalFormat: inputFormat, targetFormat: targetFormat)
        }
        
        print("âœ… [AIVoice] Audio engine configured for streaming")
    }
    
    func initializeAIConversation(with matchResult: MatchResult) async {
        print("ğŸ¤– [AIVoice] Initializing AI conversation for session: \(generateSessionId())")
        
        self.matchContext = matchResult
        
        // è®¾ç½®å¯¹è¯ä¸Šä¸‹æ–‡ - AIåº”è¯¥æ˜¯èŠå¤©ä¼™ä¼´ï¼Œä¸æ˜¯åŒ¹é…ç®—æ³•
        conversationContext = """
        You are a friendly AI conversation partner in a voice chat app. The user wants to discuss these topics: \(matchResult.topics.joined(separator: ", ")).
        
        Their original message was: "\(matchResult.transcription)"
        
        Key guidelines:
        - You are NOT a matching algorithm or service
        - You are a conversation partner who enjoys discussing these topics
        - Keep responses conversational and engaging
        - Ask follow-up questions to keep the conversation flowing
        - Use natural, spoken language (this is voice chat)
        - Don't mention "finding matches" or "waiting for others"
        - Focus on having an interesting discussion about the topics
        
        Hashtags for context: \(matchResult.hashtags.joined(separator: ", "))
        """
        
        print("ğŸ§  [AIVoice] AI conversation context set:")
        print("   ğŸ“ Transcription: \(matchResult.transcription)")
        print("   ğŸ·ï¸ Topics: \(matchResult.topics)")
        print("   #ï¸âƒ£ Hashtags: \(matchResult.hashtags)")
        
        // è¿æ¥åˆ° WebSocket
        print("ğŸ”Œ [AIVoice] Connecting to GPT-4o Realtime API...")
        print("ğŸ¯ [AIVoice] Will send conversation context about: \(matchResult.topics)")
        
        await connectToRealtimeAPI()
        
        print("âœ… [AIVoice] AI conversation initialized with topic context")
    }
    
    private func connectToRealtimeAPI() async {
        guard let token = authToken else {
            print("âŒ [AIVoice] No auth token available")
            return
        }
        
        print("ğŸ”Œ [AIVoice] Connecting to GPT-4o Audio Stream API...")
        print("ğŸ¯ [AIVoice] Will send conversation context about: \(matchContext?.topics ?? [])")
        
        await MainActor.run {
            // åˆ›å»º WebSocket æœåŠ¡
            webSocketService = WebSocketService()
            webSocketService?.delegate = self
            
            // è¿æ¥åˆ°æ–°çš„éŸ³é¢‘æµç«¯ç‚¹
            webSocketService?.connect(to: APIConfig.WebSocket.aiAudioStream, with: token)
        }
    }
    
    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer, originalFormat: AVAudioFormat, targetFormat: AVAudioFormat) {
        guard sessionStarted && !isMuted && isRecording else { 
            // print("ğŸ”‡ [AIVoice] Skipping audio - sessionStarted: \(sessionStarted), muted: \(isMuted), recording: \(isRecording)")
            return 
        }
        
        // ğŸ”‘ SERVER-SIDE VAD: No client-side speech detection needed!
        // OpenAI's server will handle voice activity detection automatically
        let audioLevel = calculateAudioLevel(buffer)
        
        // åªè®°å½•éŸ³é¢‘ç”µå¹³ç”¨äºè°ƒè¯•ï¼Œä¸åšè¯­éŸ³æ£€æµ‹
        if audioLevel > 0.001 {
            print("ğŸ¤ [AIVoice] Audio level: \(audioLevel) - SERVER VAD ENABLED")
        }
        
        print("ğŸ¤ [AIVoice] Processing audio buffer - frames: \(buffer.frameLength), level: \(audioLevel) - using server-side VAD")
        
        // åˆ›å»ºéŸ³é¢‘è½¬æ¢å™¨
        guard let converter = AVAudioConverter(from: originalFormat, to: targetFormat) else {
            print("âŒ [AIVoice] Failed to create audio converter")
            return
        }
        
        // åˆ›å»ºè¾“å‡ºç¼“å†²åŒº
        let outputFrameCapacity = AVAudioFrameCount(Double(buffer.frameLength) * targetFormat.sampleRate / originalFormat.sampleRate)
        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: outputFrameCapacity) else {
            print("âŒ [AIVoice] Failed to create output buffer")
            return
        }
        
        // æ‰§è¡Œæ ¼å¼è½¬æ¢
        var error: NSError?
        _ = converter.convert(to: outputBuffer, error: &error) { _, outStatus in
            outStatus.pointee = .haveData
            return buffer
        }
        
        if let error = error {
            print("âŒ [AIVoice] Audio conversion failed: \(error)")
            return
        }
        
        // è½¬æ¢ä¸ºData
        guard let audioData = outputBuffer.toData() else {
            print("âŒ [AIVoice] Failed to convert converted buffer to data")
            return
        }
        
        print("ğŸµ [AIVoice] Audio converted successfully: \(audioData.count) bytes (from \(buffer.frameLength) frames)")
        
        // ç¼–ç ä¸ºbase64å¹¶å‘é€
        let base64Audio = audioData.base64EncodedString()
        
        // è®°å½•é¦–æ¬¡éŸ³é¢‘æ—¶é—´ï¼ˆä»…ç”¨äºè°ƒè¯•ï¼‰
        if audioStartTime == nil {
            audioStartTime = Date()
            print("â±ï¸ [AIVoice] First audio chunk - server VAD will handle turn detection")
        }
        
        let audioMessage: [String: Any] = [
            "type": "audio_chunk",
            "audio_data": base64Audio,
            "language": "en-US",
            "timestamp": Date().timeIntervalSince1970
        ]
        
        audioChunkIndex += 1
        webSocketService?.send(audioMessage)
        print("ğŸ“¤ [AIVoice] Sent audio chunk #\(audioChunkIndex): \(audioData.count) bytes (\(base64Audio.count) base64 chars)")
    }
    
    private func calculateAudioLevel(_ buffer: AVAudioPCMBuffer) -> Float {
        guard let channelData = buffer.floatChannelData?[0] else { return 0.0 }
        
        var maxLevel: Float = 0.0
        var rmsSum: Float = 0.0
        let frameCount = Int(buffer.frameLength)
        
        // è®¡ç®—RMSï¼ˆå‡æ–¹æ ¹ï¼‰å’Œå³°å€¼ç”µå¹³
        for i in 0..<frameCount {
            let sample = abs(channelData[i])
            maxLevel = max(maxLevel, sample)
            rmsSum += sample * sample
        }
        
        let rmsLevel = sqrt(rmsSum / Float(frameCount))
        
        // ç»“åˆRMSå’Œå³°å€¼ï¼Œç»™äºˆRMSæ›´å¤§æƒé‡
        return (rmsLevel * 0.7 + maxLevel * 0.3)
    }
    
    private func triggerUtteranceEnd() {
        print("ğŸ”š [AIVoice] triggerUtteranceEnd called - hasDetectedSpeech: \(hasDetectedSpeech)")
        guard hasDetectedSpeech else { 
            print("ğŸ”š [AIVoice] No speech detected, skipping utterance end")
            return 
        }
        
        print("ğŸ”š [AIVoice] âœ… Triggering utterance end...")
        
        let utteranceEndMessage: [String: Any] = [
            "type": "utterance_end",
            "timestamp": Date().timeIntervalSince1970
        ]
        
        if webSocketService != nil {
            webSocketService?.send(utteranceEndMessage)
            print("ğŸ”š [AIVoice] âœ… Utterance end message sent to backend")
            print("ğŸ”š [AIVoice] Utterance end sent:", utteranceEndMessage)
            print("ğŸ”š [AIVoice] Current state - isRecording: \(isRecording), sessionStarted: \(sessionStarted)")
        } else {
            print("ğŸ”š [AIVoice] âŒ WebSocket service is nil!")
        }
        
        // é‡ç½®çŠ¶æ€å’Œè®¡æ—¶å™¨
        resetAudioState()
    }
    
    private func forceUtteranceEnd() {
        print("ğŸš¨ [AIVoice] Force utterance end triggered - safety mechanism")
        
        let utteranceEndMessage: [String: Any] = [
            "type": "utterance_end",
            "timestamp": Date().timeIntervalSince1970,
            "reason": "force_timeout"
        ]
        
        if webSocketService != nil {
            webSocketService?.send(utteranceEndMessage)
            print("ğŸš¨ [AIVoice] âœ… Force utterance end message sent to backend")
            print("ğŸš¨ [AIVoice] Force utterance end sent:", utteranceEndMessage)
            print("ğŸš¨ [AIVoice] Current state - isRecording: \(isRecording), sessionStarted: \(sessionStarted)")
        }
        
        // é‡ç½®æ‰€æœ‰çŠ¶æ€å’Œè®¡æ—¶å™¨
        resetAudioState()
    }
    
    private func resetAudioState() {
        hasDetectedSpeech = false
        speechStartTime = nil
        audioStartTime = nil
        audioChunkIndex = 0
        consecutiveSpeechFrames = 0
        consecutiveSilenceFrames = 0
        
        // æ¸…ç†æ‰€æœ‰è®¡æ—¶å™¨
        silenceTimer?.invalidate()
        silenceTimer = nil
        maxUtteranceTimer?.invalidate()
        maxUtteranceTimer = nil
        forceUtteranceTimer?.invalidate()
        forceUtteranceTimer = nil
        
        print("ğŸ”„ [AIVoice] All audio state and timers reset (chunk index reset to 0)")
    }
    
    private func sendStartSession() {
        let startSessionMessage: [String: Any] = [
            "type": "start_session",
            "user_context": [
                "topics": matchContext?.topics ?? [],
                "hashtags": matchContext?.hashtags ?? [],
                "transcription": matchContext?.transcription ?? "",
                "conversation_context": conversationContext
            ],
            "timestamp": Date().timeIntervalSince1970
        ]
        
        webSocketService?.send(startSessionMessage)
        print("ğŸ“¤ [AIVoice] Sent start session with topic context")
    }
    
    func startListening() async {
        guard sessionStarted && !isMuted else {
            print("âš ï¸ [AIVoice] Cannot start listening - session not started or muted")
            print("âš ï¸ [AIVoice] sessionStarted: \(sessionStarted), isMuted: \(isMuted)")
            return
        }
        
        print("ğŸ¤ [AIVoice] Starting continuous voice listening with audio engine")
        print("ğŸ¤ [AIVoice] Current state - sessionStarted: \(sessionStarted), isMuted: \(isMuted)")
        
        await MainActor.run {
            isListening = true
        }
        
        startAudioEngine()
    }
    
    func toggleMute() {
        isMuted.toggle()
        print("ğŸ”‡ [AIVoice] Audio input \(isMuted ? "muted" : "unmuted")")
        print("ğŸ”‡ [AIVoice] New state - isMuted: \(isMuted), isRecording: \(isRecording)")
        
        if isMuted {
            stopAudioEngine()
        } else if sessionStarted {
            startAudioEngine()
        }
    }
    
    private func startAudioEngine() {
        guard !isMuted, let audioEngine = audioEngine else { 
            print("âš ï¸ [AIVoice] Cannot start audio engine - muted: \(isMuted), engine exists: \(audioEngine != nil)")
            return 
        }
        
        do {
            // ç¡®ä¿å¼•æ“å·²å‡†å¤‡å¥½
            if !audioEngine.isRunning {
                print("ğŸµ [AIVoice] Preparing and starting audio engine...")
                audioEngine.prepare()
                try audioEngine.start()
                isRecording = true
                print("ğŸ™ï¸ [AIVoice] âœ… Audio engine started for streaming - isRecording: \(isRecording)")
            } else {
                print("ğŸ™ï¸ [AIVoice] Audio engine already running")
            }
        } catch {
            print("âŒ [AIVoice] Failed to start audio engine: \(error)")
        }
    }
    
    private func stopAudioEngine() {
        guard let audioEngine = audioEngine else { return }
        
        if audioEngine.isRunning {
            audioEngine.stop()
            isRecording = false
            
            // æ¸…ç†è¯­éŸ³æ£€æµ‹çŠ¶æ€
            resetAudioState()
            
            Task { @MainActor in
                isListening = false
            }
            
            print("â¹ï¸ [AIVoice] Audio engine stopped, state cleared")
        }
    }
    
    deinit {
        // æ¸…ç†éŸ³é¢‘å¼•æ“
        stopAudioEngine()
        inputNode?.removeTap(onBus: 0)
        silenceTimer?.invalidate()
        print("ğŸ§¹ [AIVoice] Audio service cleaned up")
    }
    
    // ç”ŸæˆåŸºäºè¯é¢˜çš„ AI å“åº”ï¼ˆå½“å‰æ˜¯æ¨¡æ‹Ÿï¼ŒçœŸå®å®ç°ä¼šæ¥è‡ª GPT-4oï¼‰
    func generateTopicBasedResponse() -> String {
        guard let context = matchContext else {
            return "Let's continue our conversation!"
        }
        
        let topic = context.topics.first ?? "this topic"
        let responses = [
            "That's fascinating! What specifically interests you about \(topic)?",
            "I'd love to hear more about your experience with \(topic).",
            "What got you started with \(topic)? Any interesting stories?",
            "Have you seen any recent developments in \(topic) that caught your attention?",
            "What aspects of \(topic) do you think others should know about?"
        ]
        
        return responses.randomElement() ?? "Tell me more about \(topic)!"
    }
    
    // MARK: - Helper æ–¹æ³•
    
    private func generateSessionId() -> String {
        return "ai_waiting_\(UUID().uuidString)_\(Date().timeIntervalSince1970)"
    }
    
    // MARK: - WebSocketDelegate
    
    func webSocketDidConnect(_ service: WebSocketService) {
        print("âœ… [AIVoice] WebSocket connected to GPT-4o Realtime")
        
        DispatchQueue.main.async {
            self.isConnected = true
        }
        
        // å‘é€è®¤è¯æ¶ˆæ¯
        let authMessage: [String: Any] = [
            "type": "auth",
            "token": authToken ?? ""
        ]
        
        service.send(authMessage)
        print("ğŸ“¤ [AIVoice] Sent authentication")
    }
    
    func webSocketDidDisconnect(_ service: WebSocketService) {
        print("âŒ [AIVoice] WebSocket disconnected")
        
        DispatchQueue.main.async {
            self.isConnected = false
            self.isListening = false
        }
    }
    
    func webSocket(_ service: WebSocketService, didReceiveMessage message: [String: Any]) {
        print("ğŸ“¥ [AIVoice] Received message: \(message["type"] as? String ?? "unknown")")
        
        guard let type = message["type"] as? String else { return }
        
        switch type {
        case "authenticated":
            print("âœ… [AIVoice] Authenticated with backend")
            // åªåœ¨ç¬¬ä¸€æ¬¡è®¤è¯æ—¶å¼€å§‹ä¼šè¯
            if !isAuthenticated {
                sendStartSession()
                isAuthenticated = true
            }
            
        case "session_started":
            print("âœ… [AIVoice] Session started")
            print("ğŸ¯ [AIVoice] Session message data: \(message)")
            sessionStarted = true
            print("âœ… [AIVoice] sessionStarted flag set â†’", sessionStarted)
            
            // ä¼šè¯å¼€å§‹åè‡ªåŠ¨å¼€å§‹ç›‘å¬
            print("ğŸš€ [AIVoice] About to start listening...")
            print("ğŸš€ [AIVoice] Current state before startListening - sessionStarted: \(sessionStarted), isMuted: \(isMuted)")
            Task {
                await startListening()
            }
            
        case "stt_chunk":
            // ç®€åŒ–ï¼šä¸æ˜¾ç¤ºéƒ¨åˆ†è½¬å†™ï¼Œé¿å…UIé—ªçƒ
            if let text = message["text"] as? String {
                print("ğŸ¤ğŸ“ [AIVoice] Partial: '\(text)'")
            }
            
        case "stt_done":
            print("âœ… [AIVoice] Complete transcription received")
            if let text = message["text"] as? String {
                print("ğŸ“âœ… [AIVoice] You said: '\(text)'")
                // æ¸…ç©ºæ˜¾ç¤ºï¼Œå‡†å¤‡æ¥æ”¶AIå›å¤
                DispatchQueue.main.async {
                    self.currentResponse = ""
                }
            }
            
        // ç®€åŒ–ï¼šåˆ æ‰æ—§çš„ai_responseå¤„ç†ï¼Œç°åœ¨ç”¨response.text.delta
        // ç®€åŒ–ï¼šåˆ æ‰æ—§çš„audio_responseå¤„ç†ï¼Œç°åœ¨ç”¨audio_chunk
            
        case "audio_chunk":
            print("ğŸ”Š [AIVoice] Received real-time audio chunk")
            if let audioData = message["audio"] as? String {
                print("ğŸ”ŠğŸµ [AIVoice] Real-time audio chunk: \(audioData.count) base64 chars")
                // For real-time chunks, you might want to accumulate them or play immediately
                playAudioResponse(audioData)
            }
            
        case "utterance_end":
            print("âœ… [AIVoice] Utterance ended")
            // å½“è¯­éŸ³æ´»åŠ¨ç»“æŸæ—¶ï¼Œé‡ç½®è¯­éŸ³æ£€æµ‹çŠ¶æ€
            resetAudioState()
            lastAudioTime = Date()
            print("ğŸ”Š [AIVoice] Audio state reset, lastAudioTime updated")
            
        // GPT-4o Realtime API æ ‡å‡†äº‹ä»¶
        case "response.audio.delta":
            print("ğŸ”Š [AIVoice] Received GPT-4o audio delta")
            if let audioData = message["delta"] as? String {
                print("ğŸ”ŠğŸµ [AIVoice] GPT-4o audio delta: \(audioData.count) base64 chars")
                playAudioResponse(audioData)
            }
            
        case "response.text.delta":
            print("ğŸ“ [AIVoice] Received GPT-4o text delta")
            if let textDelta = message["delta"] as? String {
                print("ğŸ“ğŸ¤– [AIVoice] GPT-4o text: '\(textDelta)'")
                DispatchQueue.main.async {
                    self.currentResponse += textDelta  // ç®€å•ç´¯åŠ æ–‡å­—æ˜¾ç¤º
                }
            }
            
        case "response.done":
            print("âœ… [AIVoice] GPT-4o response completed")
            // ç®€åŒ–ï¼šä¸éœ€è¦å¤æ‚çš„çŠ¶æ€ç®¡ç†ï¼Œè®©AIæŒç»­ç›‘å¬
            
        case "audio_received":
            print("ğŸ“¥ [AIVoice] Backend acknowledgment - audio received")
            if let chunksAccumulated = message["chunks_accumulated"] as? Int {
                print("ğŸµ [AIVoice] Audio chunks accumulated: \(chunksAccumulated)")
            }
            
        case "error":
            print("âŒ [AIVoice] WebSocket error: \(message["message"] as? String ?? "unknown")")
            print("âŒ [AIVoice] Full error message: \(message)")
            
        default:
            print("â“ [AIVoice] Unknown message type: \(type)")
            print("â“ [AIVoice] Full unknown message: \(message)")
        }
    }
    
    private func playAudioResponse(_ audioData: String) {
        // è§£ç å¹¶æ’­æ”¾AIçš„éŸ³é¢‘å›åº”
        guard let data = Data(base64Encoded: audioData) else {
            print("âŒ [AIVoice] Failed to decode audio data")
            return
        }
        
        do {
            audioPlayer = try AVAudioPlayer(data: data)
            audioPlayer?.delegate = self
            audioPlayer?.play()
            
            DispatchQueue.main.async {
                self.isAISpeaking = true
            }
            
            print("ğŸ”Š [AIVoice] Playing AI audio response")
        } catch {
            print("âŒ [AIVoice] Failed to play audio: \(error)")
        }
    }
    
    func webSocket(_ service: WebSocketService, didEncounterError error: Error) {
        print("âŒ [AIVoice] WebSocket error: \(error)")
        
        DispatchQueue.main.async {
            self.isConnected = false
            self.isListening = false
        }
    }
}

// MARK: - AVAudioPlayerDelegate
extension AIVoiceService {
    func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
        DispatchQueue.main.async {
            self.isAISpeaking = false
        }
        print("ğŸ”Š [AIVoice] AI finished speaking")
    }
}

struct VoicePinchedCircle: View {
    var level: CGFloat
    private let baseDiameter: CGFloat = 34
    private let growth: CGFloat = 38

    var body: some View {
        Capsule()
            .fill(Color(white: 0.93))
            .frame(width: baseDiameter,
                   height: baseDiameter + level * growth)
            .shadow(radius: 2)
    }
}

// ä¿æŒå‘åå…¼å®¹çš„é¢„è§ˆ
struct UserVoiceInput: View {
    var body: some View {
        UserVoiceTopicMatchingView(matchResult: MatchResult(
            transcription: "I want to talk about AI",
            topics: ["Artificial Intelligence", "Technology"],
            hashtags: ["#AI", "#Tech"],
            matchId: "preview_match",
            sessionId: "preview_session",
            confidence: 0.8,
            waitTime: 30
        ))
    }
}

#Preview {
    UserVoiceInput()
}

// MARK: - Extensions
extension AVAudioPCMBuffer {
    func toData() -> Data? {
        let audioBuffer = audioBufferList.pointee.mBuffers
        let data = Data(bytes: audioBuffer.mData!, count: Int(audioBuffer.mDataByteSize))
        return data
    }
}
